# Multimodal Archive Retrieval System

<div align="center">

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-Latest-red.svg)](https://pytorch.org/)
[![CLIP](https://img.shields.io/badge/Model-CLIP-green.svg)](https://github.com/openai/CLIP)
[![FAISS](https://img.shields.io/badge/Search-FAISS-orange.svg)](https://github.com/facebookresearch/faiss)
[![Streamlit](https://img.shields.io/badge/UI-Streamlit-ff4b4b.svg)](https://streamlit.io/)

An AI-powered news-style multimodal archive system for storing and retrieving **images, videos, audio, and text content**.

[Demo](#-demo) ‚Ä¢ [Features](#-features) ‚Ä¢ [Installation](#-installation) ‚Ä¢ [Usage](#-usage) ‚Ä¢ [Contributing](#-contributing)

</div>

## üöÄ Overview

This project prototypes an AI-powered multimodal archive system similar to what news channels and media organizations use for content retrieval. The system enables **semantic similarity search** across different modalities, allowing users to find relevant content regardless of input type.

**Key Innovation**: Query with any modality (image, video, audio, text) and retrieve similar content across the entire archive using deep learning embeddings and vector similarity search.

### Core Capabilities

- **üñºÔ∏è Image Search** ‚Üí Find visually similar images using CLIP embeddings  
- **üé• Video Search** ‚Üí Retrieve relevant videos via keyframe analysis  
- **üéµ Audio Search** ‚Üí Match audio clips using Whisper/Wav2Vec2 embeddings  
- **üìù Text Search** ‚Üí Semantic search through captions and descriptions  
- **üîÑ Cross-Modal Retrieval** ‚Üí Query with one format, get results from all formats  

## üéØ Use Cases

- **News Archives**: Rapid retrieval of related footage and assets for story development  
- **Media Production**: Finding B-roll, stock footage, and related content  
- **Content Management**: Organizing multimedia databases with semantic search  
- **Research Applications**: Academic analysis of multimodal datasets  

## üìä Dataset

**MSR-VTT (Microsoft Research Video to Text)** - Prototype dataset includes:  
- **10,000** short YouTube video clips (~20 seconds each)  
- **200,000** human-written captions (20 per video)  
- **20 comprehensive categories** covering diverse visual content  
- **Multimodal data**: Video frames, audio tracks, and text annotations  

## üõ†Ô∏è Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Deep Learning** | PyTorch, Transformers | Model inference and embedding generation |
| **Multimodal Models** | CLIP, Whisper, Wav2Vec2 | Cross-modal understanding and feature extraction |
| **Vector Search** | FAISS | High-speed similarity search and indexing |
| **Video Processing** | MoviePy, OpenCV | Frame extraction and preprocessing |
| **Audio Processing** | librosa, torchaudio | Audio feature extraction |
| **Web Interface** | Streamlit | Interactive frontend |
| **Data Management** | pandas, numpy | Data preprocessing and operations |

## ‚ö° Quick Start

### Prerequisites

- Python 3.8+  
- CUDA-capable GPU (recommended)  
- 8GB+ RAM  

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/multimodal-archive-retrieval.git
   cd multimodal-archive-retrieval
